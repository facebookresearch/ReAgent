{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import petastorm\n",
    "from os.path import expanduser, join, abspath\n",
    "from petastorm.etl.dataset_metadata import materialize_dataset\n",
    "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
    "from pyspark.sql.types import StructType, ArrayType, IntegerType, LongType, StringType, DoubleType, MapType\n",
    "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
    "import numpy as np \n",
    "from petastorm.pytorch import DataLoader\n",
    "\n",
    "# PARAMS\n",
    "num_epochs = 1\n",
    "batch_size = 64\n",
    "row_group_size_mb = 64\n",
    "num_partitions = 10\n",
    "ONE_HOT_TYPE = np.int64\n",
    "\n",
    "# where to put the petastorm parquet files\n",
    "output_url = \"file:///Users/kaiwenw/Desktop/ReAgent/my_petastorm_output\"\n",
    "\n",
    "# TODO: The below two should be stored in a config.\n",
    "# list of all the actions (for one-hot encoding)\n",
    "actions = ['0', '1']\n",
    "\n",
    "# list of all the features and their possible values\n",
    "feature_map = {\n",
    "    'state_features': [0,1,2,3],\n",
    "    'next_state_features': [0,1,2,3],\n",
    "    'metrics': ['reward'],\n",
    "}\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[1]') \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyspark_type_to_petastorm_type(t):\n",
    "    \"\"\" scalar type conversions \"\"\"\n",
    "    if isinstance(t, DoubleType):\n",
    "        return np.float64\n",
    "    elif isinstance(t, IntegerType): \n",
    "        return np.int32\n",
    "    elif isinstance(t, LongType):\n",
    "        return np.int64\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "def get_petastorm_schema(actions, feature_map, sparse_pyspark_schema):\n",
    "    \"\"\"\n",
    "    Does two functions:\n",
    "    1) handles action/action_mask \n",
    "    2) performs sparse2dense\n",
    "    Every scalar field should stay as is. \n",
    "    Every Map field should become dense (array). We assume that they are in features.\n",
    "    They have a corresponding presence array.\n",
    "    \"\"\"\n",
    "    unischema_fields = []\n",
    "    def add_field(name, petastorm_type, shape, codec):\n",
    "        # nothing can be null\n",
    "        unischema_fields.append(\n",
    "            UnischemaField(name, petastorm_type, shape, codec, False)\n",
    "        )\n",
    "    for struct_field in sparse_pyspark_schema:\n",
    "        # first handle actions and action masks\n",
    "        if struct_field.name in [\"action\", \"next_action\"]:\n",
    "            add_field(\n",
    "                name=struct_field.name,\n",
    "                petastorm_type=ONE_HOT_TYPE,\n",
    "                shape=(),\n",
    "                codec=ScalarCodec(LongType()),\n",
    "            )\n",
    "        elif struct_field.name in [\"possible_actions\", \"possible_next_actions\"]:\n",
    "            add_field(\n",
    "                name=struct_field.name,\n",
    "                petastorm_type=ONE_HOT_TYPE,\n",
    "                shape=(len(actions), ),\n",
    "                codec=NdarrayCodec(),\n",
    "            )\n",
    "        # now perform sparse2dense\n",
    "        elif isinstance(struct_field.dataType, MapType):\n",
    "            val_type = struct_field.dataType.valueType\n",
    "            assert not isinstance(val_type, MapType), f\"{struct_field.name} has Map type with value type of Map\"\n",
    "            # add presence array\n",
    "            add_field(\n",
    "                name=f\"{struct_field.name}_presence\",\n",
    "                petastorm_type=np.int64,\n",
    "                shape=(len(feature_map[struct_field.name]), ),\n",
    "                codec=NdarrayCodec(),\n",
    "            )\n",
    "            # add dense array\n",
    "            # also assume that mapped values are scalars\n",
    "            add_field(\n",
    "                name=struct_field.name,\n",
    "                petastorm_type=pyspark_type_to_petastorm_type(val_type),\n",
    "                shape=(len(feature_map[struct_field.name]), ),\n",
    "                codec=NdarrayCodec(),\n",
    "            )\n",
    "        else:\n",
    "            assert not isinstance(struct_field.dataType, ArrayType), f\"{struct_field.name} has array type\"\n",
    "            # simply add scalar field\n",
    "            add_field(\n",
    "                name=struct_field.name,\n",
    "                petastorm_type=pyspark_type_to_petastorm_type(struct_field.dataType),\n",
    "                shape=(),\n",
    "                codec=ScalarCodec(struct_field.dataType),\n",
    "            )\n",
    "            \n",
    "    return Unischema(\"TimelineSchema\", unischema_fields)\n",
    "\n",
    "def preprocessing(actions, feature_map, schema):\n",
    "    \"\"\" \n",
    "    Does two functions:\n",
    "        1) handles action/action_mask \n",
    "        2) performs sparse2dense\n",
    "    \"\"\"\n",
    "    def get_schema_type(name):\n",
    "        return getattr(schema, name).numpy_dtype\n",
    "    \n",
    "    def find_action(desired_action):\n",
    "        for i, a in enumerate(actions):\n",
    "            if a == desired_action:\n",
    "                return i\n",
    "        return len(actions)\n",
    "\n",
    "    def row_map(row):\n",
    "        row_dict = row.asDict()\n",
    "        # first handle the action/masks\n",
    "        action_keys = [\"action\", \"next_action\"]\n",
    "        for k in action_keys:\n",
    "            row_dict[k] = find_action(row_dict[k])\n",
    "            \n",
    "        possible_action_keys = [\"possible_actions\", \"possible_next_actions\"]\n",
    "        for k in possible_action_keys:\n",
    "            mask = np.zeros(len(actions), dtype=ONE_HOT_TYPE)\n",
    "            for a in row_dict[k]:\n",
    "                i = find_action(a)\n",
    "                assert i < len(actions)\n",
    "                mask[i] = 1\n",
    "            row_dict[k] = mask\n",
    "        \n",
    "        # now handle rest of the keys (including sparse2dense)\n",
    "        rest_keys = row_dict.keys() - set(action_keys + possible_action_keys)\n",
    "        for k in row_dict.keys() - set(action_keys + possible_action_keys):\n",
    "            val_type = get_schema_type(k)\n",
    "            val = row_dict[k]\n",
    "            # convert sparse to dense\n",
    "            if isinstance(val, dict):\n",
    "                presence_arr = []\n",
    "                dense_arr = []\n",
    "                for feature in feature_map[k]:\n",
    "                    # absent\n",
    "                    if feature not in val:\n",
    "                        presence_arr.append(0)\n",
    "                        dense_arr.append(0.0) # TODO: assuming value type is a number\n",
    "                    # present\n",
    "                    else:\n",
    "                        presence_arr.append(1)\n",
    "                        dense_arr.append(val[feature])\n",
    "                presence_key = f\"{k}_presence\"\n",
    "                row_dict[presence_key] = np.array(presence_arr, dtype=get_schema_type(presence_key))\n",
    "                row_dict[k] = np.array(dense_arr, dtype=val_type)\n",
    "            # scalar\n",
    "            else:\n",
    "                assert not isinstance(val, list)\n",
    "                row_dict[k] = val\n",
    "        return dict_to_spark_row(schema, row_dict)\n",
    "    \n",
    "    return row_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM cartpole_discrete_training\").drop(\"ds\", \"mdp_id\")\n",
    "schema = get_petastorm_schema(actions, feature_map, df.schema)\n",
    "\n",
    "with materialize_dataset(spark, output_url, schema, row_group_size_mb):\n",
    "    rdd = df.rdd.map(preprocessing(actions, feature_map, schema))\n",
    "    out_df = spark.createDataFrame(rdd, schema.as_spark_schema()).coalesce(num_partitions)\n",
    "    out_df.write.mode('overwrite').parquet(output_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "from petastorm import make_reader, make_batch_reader\n",
    "\n",
    "reader = make_reader(output_url, num_epochs=num_epochs)\n",
    "with DataLoader(reader, batch_size=batch_size) as train_loader:\n",
    "    for idx, row in enumerate(train_loader):\n",
    "        print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read as rdd\n",
    "from petastorm.spark_utils import dataset_as_rdd\n",
    "rdd = dataset_as_rdd(output_url, spark, [field for name, field in schema.fields.items()])\n",
    "rdd.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimelineSchema_view_view(action=0, action_probability=0.975, metrics=array([1.]), metrics_presence=array([1]), next_action=0, next_state_features=array([ 0.04426776,  0.04691078, -0.02820231,  0.0002014 ]), next_state_features_presence=array([1, 1, 1, 1]), possible_actions=array([1, 1]), possible_next_actions=array([1, 1]), reward=1.0, sequence_number=5, sequence_number_ordinal=6, state_features=array([ 0.03943367,  0.24170479, -0.02249626, -0.28530234]), state_features_presence=array([1, 1, 1, 1]), time_diff=1, time_since_first=5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
