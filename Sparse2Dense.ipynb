{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import petastorm\n",
    "from os.path import expanduser, join, abspath\n",
    "from petastorm.etl.dataset_metadata import materialize_dataset\n",
    "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
    "from pyspark.sql.types import StructType, ArrayType, IntegerType, LongType, StringType, DoubleType, MapType\n",
    "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
    "import numpy as np \n",
    "from petastorm.pytorch import DataLoader\n",
    "\n",
    "# PARAMS\n",
    "num_epochs = 1\n",
    "batch_size = 64\n",
    "row_group_size_mb = 64\n",
    "num_partitions = 10\n",
    "ONE_HOT_TYPE = np.int64\n",
    "\n",
    "# where to put the petastorm parquet files\n",
    "output_url = \"file:///Users/kaiwenw/Desktop/ReAgent/my_petastorm_output\"\n",
    "\n",
    "# TODO: The below two should be stored in a config.\n",
    "# list of all the actions (for one-hot encoding)\n",
    "actions = ['0', '1']\n",
    "\n",
    "# list of all the features and their possible values\n",
    "feature_map = {\n",
    "    'state_features': [0,1,2,3],\n",
    "    'next_state_features': [0,1,2,3],\n",
    "    'metrics': ['reward'],\n",
    "}\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[1]') \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyspark_type_to_petastorm_type(t):\n",
    "    \"\"\" scalar type conversions \"\"\"\n",
    "    if isinstance(t, DoubleType):\n",
    "        return np.float64\n",
    "    elif isinstance(t, IntegerType): \n",
    "        return np.int32\n",
    "    elif isinstance(t, LongType):\n",
    "        return np.int64\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "def get_petastorm_schema(actions, feature_map, sparse_pyspark_schema):\n",
    "    \"\"\"\n",
    "    Does two functions:\n",
    "    1) handles action/action_mask \n",
    "    2) performs sparse2dense\n",
    "    Every scalar field should stay as is. \n",
    "    Every Map field should become dense (array). We assume that they are in features.\n",
    "    They have a corresponding presence array.\n",
    "    \"\"\"\n",
    "    unischema_fields = []\n",
    "    def add_field(name, petastorm_type, shape, codec):\n",
    "        # nothing can be null\n",
    "        unischema_fields.append(\n",
    "            UnischemaField(name, petastorm_type, shape, codec, False)\n",
    "        )\n",
    "    for struct_field in sparse_pyspark_schema:\n",
    "        # first handle actions and action masks\n",
    "        if struct_field.name in [\"action\", \"next_action\"]:\n",
    "            add_field(\n",
    "                name=struct_field.name,\n",
    "                petastorm_type=ONE_HOT_TYPE,\n",
    "                shape=(),\n",
    "                codec=ScalarCodec(LongType()),\n",
    "            )\n",
    "        elif struct_field.name in [\"possible_actions\", \"possible_next_actions\"]:\n",
    "            add_field(\n",
    "                name=struct_field.name,\n",
    "                petastorm_type=ONE_HOT_TYPE,\n",
    "                shape=(len(actions), ),\n",
    "                codec=NdarrayCodec(),\n",
    "            )\n",
    "        # now perform sparse2dense\n",
    "        elif isinstance(struct_field.dataType, MapType):\n",
    "            val_type = struct_field.dataType.valueType\n",
    "            assert not isinstance(val_type, MapType), f\"{struct_field.name} has Map type with value type of Map\"\n",
    "            # add presence array\n",
    "            add_field(\n",
    "                name=f\"{struct_field.name}_presence\",\n",
    "                petastorm_type=np.int64,\n",
    "                shape=(len(feature_map[struct_field.name]), ),\n",
    "                codec=NdarrayCodec(),\n",
    "            )\n",
    "            # add dense array\n",
    "            # also assume that mapped values are scalars\n",
    "            add_field(\n",
    "                name=struct_field.name,\n",
    "                petastorm_type=pyspark_type_to_petastorm_type(val_type),\n",
    "                shape=(len(feature_map[struct_field.name]), ),\n",
    "                codec=NdarrayCodec(),\n",
    "            )\n",
    "        else:\n",
    "            assert not isinstance(struct_field.dataType, ArrayType), f\"{struct_field.name} has array type\"\n",
    "            # simply add scalar field\n",
    "            add_field(\n",
    "                name=struct_field.name,\n",
    "                petastorm_type=pyspark_type_to_petastorm_type(struct_field.dataType),\n",
    "                shape=(),\n",
    "                codec=ScalarCodec(struct_field.dataType),\n",
    "            )\n",
    "            \n",
    "    return Unischema(\"TimelineSchema\", unischema_fields)\n",
    "\n",
    "def preprocessing(actions, feature_map, schema):\n",
    "    \"\"\" \n",
    "    Does two functions:\n",
    "        1) handles action/action_mask \n",
    "        2) performs sparse2dense\n",
    "    \"\"\"\n",
    "    def get_schema_type(name):\n",
    "        return getattr(schema, name).numpy_dtype\n",
    "    \n",
    "    def find_action(desired_action):\n",
    "        for i, a in enumerate(actions):\n",
    "            if a == desired_action:\n",
    "                return i\n",
    "        return len(actions)\n",
    "\n",
    "    def row_map(row):\n",
    "        row_dict = row.asDict()\n",
    "        # first handle the action/masks\n",
    "        action_keys = [\"action\", \"next_action\"]\n",
    "        for k in action_keys:\n",
    "            row_dict[k] = find_action(row_dict[k])\n",
    "            \n",
    "        possible_action_keys = [\"possible_actions\", \"possible_next_actions\"]\n",
    "        for k in possible_action_keys:\n",
    "            mask = np.zeros(len(actions), dtype=ONE_HOT_TYPE)\n",
    "            for a in row_dict[k]:\n",
    "                i = find_action(a)\n",
    "                assert i < len(actions)\n",
    "                mask[i] = 1\n",
    "            row_dict[k] = mask\n",
    "        \n",
    "        # now handle rest of the keys (including sparse2dense)\n",
    "        rest_keys = row_dict.keys() - set(action_keys + possible_action_keys)\n",
    "        for k in row_dict.keys() - set(action_keys + possible_action_keys):\n",
    "            val_type = get_schema_type(k)\n",
    "            val = row_dict[k]\n",
    "            # convert sparse to dense\n",
    "            if isinstance(val, dict):\n",
    "                presence_arr = []\n",
    "                dense_arr = []\n",
    "                for feature in feature_map[k]:\n",
    "                    # absent\n",
    "                    if feature not in val:\n",
    "                        presence_arr.append(0)\n",
    "                        dense_arr.append(0.0) # TODO: assuming value type is a number\n",
    "                    # present\n",
    "                    else:\n",
    "                        presence_arr.append(1)\n",
    "                        dense_arr.append(val[feature])\n",
    "                presence_key = f\"{k}_presence\"\n",
    "                row_dict[presence_key] = np.array(presence_arr, dtype=get_schema_type(presence_key))\n",
    "                row_dict[k] = np.array(dense_arr, dtype=val_type)\n",
    "            # scalar\n",
    "            else:\n",
    "                assert not isinstance(val, list)\n",
    "                row_dict[k] = val\n",
    "        return dict_to_spark_row(schema, row_dict)\n",
    "    \n",
    "    return row_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM cartpole_discrete_training\").drop(\"ds\", \"mdp_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM cartpole_discrete_training\").drop(\"ds\", \"mdp_id\")\n",
    "schema = get_petastorm_schema(actions, feature_map, df.schema)\n",
    "\n",
    "with materialize_dataset(spark, output_url, schema, row_group_size_mb):\n",
    "    rdd = df.rdd.map(preprocessing(actions, feature_map, schema))\n",
    "    out_df = spark.createDataFrame(rdd, schema.as_spark_schema()).coalesce(num_partitions)\n",
    "    out_df.write.mode('overwrite').parquet(output_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Currently make_reader supports reading only Petastorm datasets. To read from a non-Petastorm Parquet store use make_batch_reader",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPetastormMetadataError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/petastorm/reader.py\u001b[0m in \u001b[0;36mmake_reader\u001b[0;34m(dataset_url, schema_fields, reader_pool_type, workers_count, pyarrow_serialize, results_queue_size, shuffle_row_groups, shuffle_row_drop_partitions, predicate, rowgroup_selector, num_epochs, cur_shard, shard_count, cache_type, cache_location, cache_size_limit, cache_row_size_estimate, cache_extra_settings, hdfs_driver, transform_spec)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mdataset_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_schema_from_dataset_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdfs_driver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhdfs_driver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mPetastormMetadataError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/petastorm/etl/dataset_metadata.py\u001b[0m in \u001b[0;36mget_schema_from_dataset_url\u001b[0;34m(dataset_url, hdfs_driver)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;31m# Get a unischema stored in the dataset metadata.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     \u001b[0mstored_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/petastorm/etl/dataset_metadata.py\u001b[0m in \u001b[0;36mget_schema\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    346\u001b[0m         raise PetastormMetadataError(\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0;34m'Could not find _common_metadata file. Use materialize_dataset(..) in'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m             \u001b[0;34m' petastorm.etl.dataset_metadata.py to generate this file in your ETL code.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPetastormMetadataError\u001b[0m: Could not find _common_metadata file. Use materialize_dataset(..) in petastorm.etl.dataset_metadata.py to generate this file in your ETL code. You can generate it on an existing dataset using petastorm-generate-metadata.py",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c080ee991101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"file:///Users/kaiwenw/Desktop/ReAgent/training_data/cartpole_discrete_timeline\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/petastorm/reader.py\u001b[0m in \u001b[0;36mmake_reader\u001b[0;34m(dataset_url, schema_fields, reader_pool_type, workers_count, pyarrow_serialize, results_queue_size, shuffle_row_groups, shuffle_row_drop_partitions, predicate, rowgroup_selector, num_epochs, cur_shard, shard_count, cache_type, cache_location, cache_size_limit, cache_row_size_estimate, cache_extra_settings, hdfs_driver, transform_spec)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mdataset_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_schema_from_dataset_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdfs_driver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhdfs_driver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mPetastormMetadataError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         raise RuntimeError('Currently make_reader supports reading only Petastorm datasets. '\n\u001b[0m\u001b[1;32m    135\u001b[0m                            'To read from a non-Petastorm Parquet store use make_batch_reader')\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Currently make_reader supports reading only Petastorm datasets. To read from a non-Petastorm Parquet store use make_batch_reader"
     ]
    }
   ],
   "source": [
    "from petastorm import make_reader, make_batch_reader\n",
    "output_url = \"file:///Users/kaiwenw/Desktop/ReAgent/training_data/cartpole_discrete_timeline\"\n",
    "\n",
    "reader = make_reader(output_url, num_epochs=num_epochs)\n",
    "with DataLoader(reader, batch_size=batch_size) as train_loader:\n",
    "    print(dir(train_loader))\n",
    "#     for idx, row in enumerate(train_loader):\n",
    "#         print(type(row))\n",
    "#         print(idx, row)\n",
    "#         if idx == 0:\n",
    "#             break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from petastorm.spark_utils import dataset_as_rdd\n",
    "rdds = dataset_as_rdd(output_url, spark)\n",
    "rdds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Saves RLTimelineOperator's output as Petastorm parquet files.\n",
    "\n",
    "Uses PySpark and Petastorm to convert RLTimelineOperator's output (in Hive) to\n",
    "parquet files. A key functionality is that each feature in the form of a Map\n",
    "(e.g. state_features), which Arrow can't yet handle, are converted to a dense\n",
    "array and a presence array, which is 1 iff ith feature_id is present.\n",
    "\n",
    "Additional transformations include: \n",
    "- action, which was originally a string, will be represented by the index in\n",
    "the given list of possible actions\n",
    "- possible_actions, which was originally a list of strings, will be represented\n",
    "as a bitvector mask of length len(actions).\n",
    "\"\"\"\n",
    "import pyspark\n",
    "import petastorm\n",
    "from os.path import expanduser, join, abspath\n",
    "from petastorm.etl.dataset_metadata import materialize_dataset\n",
    "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    ArrayType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    MapType,\n",
    ")\n",
    "from petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\n",
    "import numpy as np\n",
    "from petastorm.pytorch import DataLoader\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from ml.rl.workflow.helpers import parse_args\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# This is the type we use to store one-hot indices\n",
    "ONE_HOT_TYPE = np.int64\n",
    "\n",
    "\n",
    "def pyspark_to_numpy_types(pyspark_type):\n",
    "    \"\"\" \n",
    "    Converts pyspark.sql.types to their numpy equivalent.\n",
    "    \"\"\"\n",
    "    if isinstance(pyspark_type, DoubleType):\n",
    "        return np.float64\n",
    "    elif isinstance(pyspark_type, IntegerType):\n",
    "        return np.int32\n",
    "    elif isinstance(pyspark_type, LongType):\n",
    "        return np.int64\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"PySpark type {pyspark_type} does not have numpy equivalent.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_petastorm_schema(\n",
    "    actions: List[str], feature_map: Dict[str, List[Any]], sparse_pyspark_schema\n",
    ") -> Unischema:\n",
    "    \"\"\"\n",
    "    Creates the Petastorm storage schema from the schema of the\n",
    "    RLTimelineOperator's sparse output. \n",
    "\n",
    "    Args:\n",
    "        actions: List of possible actions\n",
    "        feature_map: Some features (e.g. state_features) are mappings of \n",
    "            feature_id to value. This argument is a mapping of such features to\n",
    "            the possible feature_ids that it may have. \n",
    "        sparse_pyspark_schema: Schema of RLTimelineOperator's output when loaded\n",
    "            directly by PySpark.\n",
    "\n",
    "    Returns:\n",
    "        Petastorm storage schema.\n",
    "    \"\"\"\n",
    "    unischema_fields = []\n",
    "\n",
    "    def add_field(name, petastorm_type, shape, codec):\n",
    "        # nothing can be null\n",
    "        unischema_fields.append(\n",
    "            UnischemaField(name, petastorm_type, shape, codec, False)\n",
    "        )\n",
    "\n",
    "    for struct_field in sparse_pyspark_schema:\n",
    "        # first handle actions and action masks\n",
    "        if struct_field.name in [\"action\", \"next_action\"]:\n",
    "            add_field(\n",
    "                name=struct_field.name,\n",
    "                petastorm_type=ONE_HOT_TYPE,\n",
    "                shape=(),\n",
    "                codec=ScalarCodec(LongType()),\n",
    "            )\n",
    "        elif struct_field.name in [\"possible_actions\", \"possible_next_actions\"]:\n",
    "            add_field(\n",
    "                name=struct_field.name,\n",
    "                petastorm_type=ONE_HOT_TYPE,\n",
    "                shape=(len(actions),),\n",
    "                codec=NdarrayCodec(),\n",
    "            )\n",
    "        # now perform sparse2dense\n",
    "        elif isinstance(struct_field.dataType, MapType):\n",
    "            val_type = struct_field.dataType.valueType\n",
    "            assert not isinstance(\n",
    "                val_type, MapType\n",
    "            ), f\"{struct_field.name} has Map type with value type of Map\"\n",
    "            # add presence array\n",
    "            add_field(\n",
    "                name=f\"{struct_field.name}_presence\",\n",
    "                petastorm_type=np.int64,\n",
    "                shape=(len(feature_map[struct_field.name]),),\n",
    "                codec=NdarrayCodec(),\n",
    "            )\n",
    "            # add dense array\n",
    "            # also assume that mapped values are scalars\n",
    "            add_field(\n",
    "                name=struct_field.name,\n",
    "                petastorm_type=pyspark_to_numpy_types(val_type),\n",
    "                shape=(len(feature_map[struct_field.name]),),\n",
    "                codec=NdarrayCodec(),\n",
    "            )\n",
    "        else:\n",
    "            assert not isinstance(\n",
    "                struct_field.dataType, ArrayType\n",
    "            ), f\"{struct_field.name} has array type\"\n",
    "            # simply add scalar field\n",
    "            add_field(\n",
    "                name=struct_field.name,\n",
    "                petastorm_type=pyspark_to_numpy_types(struct_field.dataType),\n",
    "                shape=(),\n",
    "                codec=ScalarCodec(struct_field.dataType),\n",
    "            )\n",
    "\n",
    "    return Unischema(\"TimelineSchema\", unischema_fields)\n",
    "\n",
    "\n",
    "def preprocessing(\n",
    "    actions: List[str], feature_map: Dict[str, List[Any]], petastorm_schema\n",
    "):\n",
    "    \"\"\" \n",
    "    Returns an RDD mapping function (mapping function on Rows) that converts \n",
    "    the RLTimelineOperator's output into the petastorm format, with actions\n",
    "    one-hot encoded, with sparse features converted to dense, and with presence\n",
    "    arrays.\n",
    "\n",
    "    Args:\n",
    "        actions: same as get_petastorm_schema\n",
    "        feature_map: same as get_petastorm_schema\n",
    "        petastorm_schema: Desired schema of the petastorm dataframe. \n",
    "    \"\"\"\n",
    "\n",
    "    def get_schema_type(feature_name):\n",
    "        \"\"\" Returns the numpy dtype associated with the feature. \"\"\"\n",
    "        assert hasattr(\n",
    "            petastorm_schema, feature_name\n",
    "        ), f\"{feature_name} does not exist.\"\n",
    "        return getattr(petastorm_schema, feature_name).numpy_dtype\n",
    "\n",
    "    def find_action_idx(desired_action):\n",
    "        \"\"\" Returns the index of action in the list of possible actions. \n",
    "        If not found in the list of possible actions, return the number of\n",
    "        possible actions.\n",
    "        \"\"\"\n",
    "        for i, a in enumerate(actions):\n",
    "            if a == desired_action:\n",
    "                return i\n",
    "        return len(actions)\n",
    "\n",
    "    def row_map(row):\n",
    "        \"\"\" The RDD mapping function \"\"\"\n",
    "        # convert Row to dict\n",
    "        row_dict = row.asDict()\n",
    "\n",
    "        # first handle one-hot encoding of action/masks\n",
    "        action_keys = [\"action\", \"next_action\"]\n",
    "        for k in action_keys:\n",
    "            row_dict[k] = find_action_idx(row_dict[k])\n",
    "\n",
    "        possible_action_keys = [\"possible_actions\", \"possible_next_actions\"]\n",
    "        for k in possible_action_keys:\n",
    "            mask = np.zeros(len(actions), dtype=ONE_HOT_TYPE)\n",
    "            for a in row_dict[k]:\n",
    "                i = find_action_idx(a)\n",
    "                assert i < len(actions), f\"action {a} (type {type(a)}) was not found in {actions}\"\n",
    "                mask[i] = 1\n",
    "            row_dict[k] = mask\n",
    "\n",
    "        # now handle rest of keys, including converting sparse to dense\n",
    "        rest_features = row_dict.keys() - set(action_keys + possible_action_keys)\n",
    "        for feature in rest_features:\n",
    "            val_type = get_schema_type(feature)\n",
    "            val = row_dict[feature]\n",
    "            # convert sparse to dense\n",
    "            if isinstance(val, dict):\n",
    "                presence_arr = []\n",
    "                dense_arr = []\n",
    "                # for every possible feature_id, check if it is part of\n",
    "                # the sparse representation. If it is, mark it as present and\n",
    "                # record it in the dense array. If it isn't, mark it\n",
    "                # as absent and record a default value in dense array.\n",
    "                for feature_id in feature_map[feature]:\n",
    "                    if feature_id in val:\n",
    "                        presence_arr.append(1)\n",
    "                        dense_arr.append(val[feature_id])\n",
    "                    else:\n",
    "                        presence_arr.append(0)\n",
    "                        # we assume values are a number here\n",
    "                        dense_arr.append(0.0)\n",
    "                presence_key = f\"{feature}_presence\"\n",
    "                row_dict[presence_key] = np.array(\n",
    "                    presence_arr, dtype=get_schema_type(presence_key)\n",
    "                )\n",
    "                row_dict[feature] = np.array(dense_arr, dtype=val_type)\n",
    "            # if not a sparse map, simply copy it\n",
    "            else:\n",
    "                assert not isinstance(val, list)\n",
    "                row_dict[feature] = val\n",
    "        return dict_to_spark_row(petastorm_schema, row_dict)\n",
    "\n",
    "    return row_map\n",
    "\n",
    "\n",
    "def save_petastorm_dataset(\n",
    "    actions, feature_map, input_table, output_table, warehouse_dir=\"spark-warehouse\", row_group_size_mb=256\n",
    "):\n",
    "    \"\"\" Assuming RLTimelineOperator output is in Hive storage at warehouse_dir,\n",
    "        save the petastorm dataset after preprocessing.\n",
    "\n",
    "        Args:\n",
    "            actions: same as get_petastorm_schema\n",
    "            feature_map: same as get_petastorm_schema\n",
    "            input_table: table to read output of RLTimelineOperator\n",
    "            output_table: location to store the dataset\n",
    "            warehouse_dir: location where RLTimelineOperator stored data\n",
    "            row_group_size_mb: parquet row group size for dataset\n",
    "    \"\"\"\n",
    "    warehouse_dir = abspath(warehouse_dir)\n",
    "    output_uri = f\"file://{abspath(output_table)}\"\n",
    "    spark = (\n",
    "        pyspark.sql.SparkSession.builder.master(\"local[1]\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    # Read dataframe, and drop unnecessary columns\n",
    "#     df = spark.sql(f\"SELECT * FROM {input_table}\")# .drop(\"ds\", \"mdp_id\")\n",
    "    df = spark.read.table(input_table).drop(\"mdp_id\")\n",
    "    peta_schema = get_petastorm_schema(actions, feature_map, df.schema)\n",
    "    with materialize_dataset(spark, output_uri, peta_schema, row_group_size_mb):\n",
    "        rdd = df.rdd.map(preprocessing(actions, feature_map, peta_schema))\n",
    "        peta_df = spark.createDataFrame(rdd, peta_schema.as_spark_schema())\n",
    "        peta_df.write.mode(\"overwrite\").parquet(output_uri)\n",
    "\n",
    "\n",
    "train_input_table = \"cartpole_discrete_training\"\n",
    "train_output_table = \"training_data/cartpole_discrete_timeline\"\n",
    "# eval_input_table = \"cartpole_discrete_eval\"\n",
    "# eval_output_table = \"training_data/cartpole_discrete_timeline_eval\"\n",
    "\n",
    "# list of all the actions (for one-hot encoding)\n",
    "actions = [0, 1]\n",
    "\n",
    "# list of all the features and their possible values\n",
    "feature_map = {\n",
    "    \"state_features\": [0, 1, 2, 3],\n",
    "    \"next_state_features\": [0, 1, 2, 3],\n",
    "    \"metrics\": [\"reward\"],\n",
    "}\n",
    "\n",
    "save_petastorm_dataset(\n",
    "    actions, feature_map, train_input_table, train_output_table\n",
    ")\n",
    "logger.info(f\"Saved training table as {train_output_table}\")\n",
    "\n",
    "# save_petastorm_dataset(\n",
    "#     actions, feature_map, eval_input_table, eval_output_table\n",
    "# )\n",
    "# logger.info(f\"Saved training table as {eval_output_table}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "table = \"cartpole_discrete_training\"\n",
    "# df = spark.sql(f\"SELECT * FROM {table}\")\n",
    "df = spark.read.table(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser, join, abspath\n",
    "import pyspark\n",
    "\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "spark = pyspark.sql.SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[1]') \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "spark.sql(\"SHOW TABLES\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = spark.read.json(\"cartpole_discrete_training\")\n",
    "# eval_df = spark.read.json(\"cartpole_discrete_eval\")\n",
    "warehouse_df = spark.sql(\"SELECT * FROM cartpole_discrete_training\")\n",
    "warehouse_eval_df = spark.sql(\"SELECT * FROM cartpole_discrete_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "warehouse_eval_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read as rdd\n",
    "from petastorm.spark_utils import dataset_as_rdd\n",
    "rdd = dataset_as_rdd(output_url, spark, [field for name, field in schema.fields.items()])\n",
    "rdd.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
