#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
"""
A network which implements a cross entropy method-based planner

The planner plans the best next action based on simulation data generated by
an ensemble of world models.

The idea is inspired by: https://arxiv.org/abs/1805.12114
"""
import itertools
import logging
import random
from typing import List, Optional, Tuple

import numpy as np
import scipy.stats as stats
import torch
import torch.nn as nn
from ml.rl import types as rlt
from ml.rl.models.base import ModelBase
from ml.rl.models.world_model import MemoryNetwork
from torch.distributions.bernoulli import Bernoulli
from torch.distributions.categorical import Categorical
from torch.distributions.normal import Normal
from torch.nn.parallel.distributed import DistributedDataParallel


logger = logging.getLogger(__name__)


class CEMPlannerNetwork(nn.Module):
    def __init__(
        self,
        mem_net_list: List[MemoryNetwork],
        cem_num_iterations: int,
        cem_population_size: int,
        ensemble_population_size: int,
        num_elites: int,
        plan_horizon_length: int,
        state_dim: int,
        action_dim: int,
        discrete_action: bool,
        terminal_effective: bool,
        gamma: float,
        alpha: float = 0.25,
        epsilon: float = 0.001,
        action_upper_bounds: Optional[np.ndarray] = None,
        action_lower_bounds: Optional[np.ndarray] = None,
    ):
        """
        :param mem_net_list: A list of world models used to simulate trajectories
        :param cem_num_iterations: The maximum number of iterations for searching the best action
        :param cem_population_size: The number of candidate solutions to evaluate in each CEM iteration
        :param ensemble_population_size: The number of trajectories to be sampled to evaluate a CEM solution
        :param num_elites: The number of elites kept to refine solutions in each iteration
        :param plan_horizon_length: The number of steps to plan ahead
        :param state_dim: state dimension
        :param action_dim: action dimension
        :param discrete_action: If actions are discrete or continuous
        :param terminal_effective: If False, planning will stop after a predicted terminal signal
        :param gamma: The reward discount factor
        :param alpha: The CEM solution update rate
        :param epsilon: The planning will stop early when the solution variance drops below epsilon
        :param action_upper_bounds: Upper bound of each action dimension.
            Only effective when discrete_action=False.
        :param action_lower_bounds: Lower bound of each action dimension.
            Only effective when discrete_action=False.
        """
        super().__init__()
        self.mem_net_list = nn.ModuleList(mem_net_list)
        self.cem_num_iterations = cem_num_iterations
        self.cem_pop_size = cem_population_size
        self.ensemble_pop_size = ensemble_population_size
        self.num_elites = num_elites
        self.plan_horizon_length = plan_horizon_length
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.terminal_effective = terminal_effective
        self.gamma = gamma
        self.alpha = alpha
        self.epsilon = epsilon
        self.discrete_action = discrete_action
        if not discrete_action:
            assert (
                (action_upper_bounds is not None)
                and (action_lower_bounds is not None)
                and (
                    action_upper_bounds.shape
                    == action_lower_bounds.shape
                    == (action_dim,)
                )
            )
            assert np.all(action_upper_bounds >= action_lower_bounds)
            self.action_upper_bounds = np.tile(
                action_upper_bounds, self.plan_horizon_length
            )
            self.action_lower_bounds = np.tile(
                action_lower_bounds, self.plan_horizon_length
            )

    @torch.no_grad()  # type: ignore
    def forward(self, input: rlt.PreprocessedState):
        assert input.state.float_features.shape == (1, self.state_dim)
        if self.discrete_action:
            return self.discrete_planning(input)
        return self.continuous_planning(input)

    @torch.no_grad()  # type: ignore
    def acc_rewards_of_one_solution(
        self, init_state: torch.Tensor, solution: torch.Tensor, solution_idx: int
    ):
        """
        ensemble_pop_size trajectories will be sampled to evaluate a
        CEM solution. Each trajectory is generated by one world model

        :param init_state: its shape is (state_dim, )
        :param solution: its shape is (plan_horizon_length, action_dim)
        :param solution_idx: the index of the solution
        :return reward: Reward of each of ensemble_pop_size trajectories
        """
        reward_matrix = np.zeros((self.ensemble_pop_size, self.plan_horizon_length))

        for i in range(self.ensemble_pop_size):
            state = init_state
            mem_net_idx = np.random.randint(0, len(self.mem_net_list))
            for j in range(self.plan_horizon_length):
                # world_model_input.state shape:
                # (1, 1, state_dim)
                # world_model_input.action shape:
                # (1, 1, action_dim)
                world_model_input = rlt.PreprocessedStateAction(
                    state=rlt.PreprocessedFeatureVector(
                        float_features=state.reshape((1, 1, self.state_dim))
                    ),
                    action=rlt.PreprocessedFeatureVector(
                        float_features=solution[j, :].reshape((1, 1, self.action_dim))
                    ),
                )
                reward, next_state, not_terminal, not_terminal_prob = self.sample_reward_next_state_terminal(
                    world_model_input, self.mem_net_list[mem_net_idx]
                )
                reward_matrix[i, j] = reward * (self.gamma ** j)

                if not not_terminal:
                    logger.debug(
                        f"Solution {solution_idx}: predict terminal at step {j}"
                        f" with prob. {1.0 - not_terminal_prob}"
                    )

                if not not_terminal:
                    break

                state = next_state

        return np.sum(reward_matrix, axis=1)

    @torch.no_grad()  # type: ignore
    def acc_rewards_of_all_solutions(
        self, input: rlt.PreprocessedState, solutions: torch.Tensor
    ) -> float:
        """
        Calculate accumulated rewards of solutions.

        :param input: the input which contains the starting state
        :param solutions: its shape is (cem_pop_size, plan_horizon_length, action_dim)
        :returns: a vector of size cem_pop_size, which is the reward of each solution
        """
        acc_reward_vec = np.zeros(self.cem_pop_size)
        init_state = input.state.float_features
        for i in range(self.cem_pop_size):
            if i % (self.cem_pop_size // 10) == 0:
                logger.debug(f"Simulating the {i}-th solution...")
            acc_reward_vec[i] = self.acc_rewards_of_one_solution(
                init_state, solutions[i], i
            )
        return acc_reward_vec

    @torch.no_grad()  # type: ignore
    def sample_reward_next_state_terminal(
        self, world_model_input: rlt.PreprocessedStateAction, mem_net: MemoryNetwork
    ):
        """ Sample one-step dynamics based on the provided world model """
        wm_output = mem_net(world_model_input)
        num_mixtures = wm_output.logpi.shape[2]
        mixture_idx = (
            Categorical(torch.exp(wm_output.logpi.view(num_mixtures)))
            .sample()
            .long()
            .item()
        )
        next_state = Normal(
            wm_output.mus[0, 0, mixture_idx], wm_output.sigmas[0, 0, mixture_idx]
        ).sample()
        reward = wm_output.reward[0, 0]
        if self.terminal_effective:
            not_terminal_prob = torch.sigmoid(wm_output.not_terminal[0, 0])
            not_terminal = Bernoulli(not_terminal_prob).sample().long().item()
        else:
            not_terminal_prob = 1.0
            not_terminal = 1
        return reward, next_state, not_terminal, not_terminal_prob

    def constrained_variance(self, mean, var):
        lb_dist, ub_dist = (
            mean - self.action_lower_bounds,
            self.action_upper_bounds - mean,
        )
        return np.minimum(np.minimum((lb_dist / 2) ** 2, (ub_dist / 2) ** 2), var)

    @torch.no_grad()  # type: ignore
    def continuous_planning(self, input: rlt.PreprocessedState) -> np.ndarray:
        # TODO: Warmstarts means and vars using previous solutions (T48841404)
        mean = (self.action_upper_bounds + self.action_lower_bounds) / 2
        var = (self.action_upper_bounds - self.action_lower_bounds) ** 2 / 16
        normal_sampler = stats.truncnorm(  # type: ignore
            -2, 2, loc=np.zeros_like(mean), scale=np.ones_like(mean)
        )

        for i in range(self.cem_num_iterations):
            logger.debug(f"{i}-th cem iteration.")
            const_var = self.constrained_variance(mean, var)
            solutions = (
                normal_sampler.rvs(
                    size=[self.cem_pop_size, self.action_dim * self.plan_horizon_length]
                )
                * np.sqrt(const_var)
                + mean
            )
            action_solutions = torch.from_numpy(
                solutions.reshape(
                    (self.cem_pop_size, self.plan_horizon_length, self.action_dim)
                )
            ).float()
            acc_rewards = self.acc_rewards_of_all_solutions(input, action_solutions)
            elites = solutions[np.argsort(acc_rewards)][-self.num_elites :]
            new_mean = np.mean(elites, axis=0)
            new_var = np.var(elites, axis=0)
            mean = self.alpha * mean + (1 - self.alpha) * new_mean
            var = self.alpha * var + (1 - self.alpha) * new_var

            if np.max(var) <= self.epsilon:
                break

        # Pick the first action of the optimal solution
        solution = mean[: self.action_dim]
        return torch.tensor(solution.reshape((1, -1)))

    @torch.no_grad()  # type: ignore
    def discrete_planning(self, input: rlt.PreprocessedState) -> Tuple[int, np.ndarray]:
        # For discrete actions, we use random shoots to get the best next action
        random_action_seqs = list(
            itertools.product(range(self.action_dim), repeat=self.plan_horizon_length)
        )
        random_action_seqs = random.choices(random_action_seqs, k=self.cem_pop_size)
        action_solutions = torch.zeros(
            self.cem_pop_size, self.plan_horizon_length, self.action_dim
        )
        for i, action_seq in enumerate(random_action_seqs):
            for j, act_idx in enumerate(action_seq):
                action_solutions[i, j, act_idx] = 1
        acc_rewards = self.acc_rewards_of_all_solutions(input, action_solutions)

        first_action_tally = np.zeros(self.action_dim)
        reward_tally = np.zeros(self.action_dim)
        for action_seq, acc_reward in zip(random_action_seqs, acc_rewards):
            first_action = action_seq[0]
            first_action_tally[first_action] += 1
            reward_tally[first_action] += acc_reward

        best_next_action_idx = np.nanargmax(reward_tally / first_action_tally)
        best_next_action_one_hot = torch.zeros(self.action_dim).float()
        best_next_action_one_hot[best_next_action_idx] = 1

        logger.debug(
            f"Choose action {best_next_action_idx}. Stats: {reward_tally} / {first_action_tally}"
            f" = {reward_tally/first_action_tally} "
        )
        return best_next_action_idx, best_next_action_one_hot


class CEMPlanner(ModelBase):
    def __init__(
        self,
        cem_planner_network: CEMPlannerNetwork,
        plan_horizon_length: int,
        state_dim: int,
        action_dim: int,
        discrete_action: bool,
    ):
        super().__init__()
        self.cem_planner_network = cem_planner_network
        self.plan_horizon_length = plan_horizon_length
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.discrete_action = discrete_action

    def get_distributed_data_parallel_model(self):
        return _DistributedDataParallelCEMPlanner(self)

    def input_prototype(self):
        return rlt.PreprocessedState.from_tensor(torch.randn(1, self.state_dim))

    def forward(self, input: rlt.PreprocessedState):
        output = self.cem_planner_network(input)
        if self.discrete_action:
            return rlt.PlanningPolicyOutput(
                next_best_discrete_action_idx=output[0],
                next_best_discrete_action_one_hot=output[1],
            )
        return rlt.PlanningPolicyOutput(next_best_continuous_action=output)


class _DistributedDataParallelCEMPlanner(ModelBase):
    def __init__(self, cem_planner: CEMPlanner):
        super().__init__()
        self.plan_horizon_length = cem_planner.plan_horizon_length
        self.state_dim = cem_planner.state_dim
        self.action_dim = cem_planner.action_dim
        self.discrete_action = cem_planner.discrete_action

        current_device = torch.cuda.current_device()  # type: ignore
        self.data_parallel = DistributedDataParallel(
            cem_planner.cem_planner_network,
            device_ids=[current_device],
            output_device=current_device,
        )
        self.cem_planner = cem_planner

    def input_prototype(self):
        return rlt.PreprocessedState.from_tensor(torch.randn(1, self.state_dim))

    def cpu_model(self):
        return self.cem_planner.cpu_model()

    def forward(self, input: rlt.PreprocessedState):
        output = self.data_parallel(input)
        if self.discrete_action:
            return rlt.PlanningPolicyOutput(
                next_best_discrete_action_idx=output[0],
                next_best_discrete_action_one_hot=output[1],
            )
        return rlt.PlanningPolicyOutput(next_best_continuous_action=output)
