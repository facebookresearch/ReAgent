{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexschneidman/anaconda3/envs/ope/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alexschneidman/anaconda3/envs/ope/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alexschneidman/anaconda3/envs/ope/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alexschneidman/anaconda3/envs/ope/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alexschneidman/anaconda3/envs/ope/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alexschneidman/anaconda3/envs/ope/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from pathlib import PurePath\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Iterable\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from reagent.ope.estimators.estimator import Estimator, EstimatorResult, Evaluator\n",
    "from reagent.ope.estimators.contextual_bandits_estimators import (\n",
    "    Action,\n",
    "    ActionDistribution,\n",
    "    ActionRewards,\n",
    "    BanditsEstimatorInput,\n",
    "    BanditsModel,\n",
    "    DMEstimator,\n",
    "    DoublyRobustEstimator,\n",
    "    IPSEstimator,\n",
    "    LogSample,\n",
    "    SwitchEstimator,\n",
    "    SwitchDREstimator\n",
    ")\n",
    "from reagent.ope.estimators.types import ActionSpace, Policy, Trainer\n",
    "from reagent.ope.trainers.linear_trainers import (\n",
    "    LogisticRegressionTrainer,\n",
    "    SGDClassifierTrainer,\n",
    "    TrainingData,\n",
    "    DecisionTreeTrainer,\n",
    "    LinearTrainer,\n",
    "    NNTrainer\n",
    ")\n",
    "from reagent.ope.test.multiclass_bandits import (\n",
    "    MultiClassDataRow,\n",
    "    UCIMultiClassDataset,\n",
    "    MultiClassContext,\n",
    "    MultiClassModel,\n",
    "    MultiClassPolicy,\n",
    "    evaluate_all\n",
    ")\n",
    "from reagent.ope.utils import RunningAverage, Clamper\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Settings\n",
    "\n",
    "Edit the experiments list with the names of UCI datasets given in reagent/test/data to produce results for each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "DEFAULT_ITERATIONS = 500\n",
    "TEST_ROOT_PATH = '..'\n",
    "UCI_DATASET_CONFIGS = os.path.join(TEST_ROOT_PATH, 'configs')\n",
    "MAX_METRIC_NAME_LENGTH = 20\n",
    "experiments = [\"ecoli\", \"letter_recog\", \"pendigits\", \"optdigits\", \"satimage\"]\n",
    "#experiments = [\"ecoli\"]\n",
    "\n",
    "experiment_params = []\n",
    "for exp in experiments:\n",
    "    with open(os.path.join(UCI_DATASET_CONFIGS, exp + '_config.json'), \"r\") as f:\n",
    "        params = json.load(f)\n",
    "        if \"dataset\" in params:\n",
    "            if \"file\" in params[\"dataset\"]:\n",
    "                params[\"dataset\"][\"file\"] = os.path.join(TEST_ROOT_PATH, params[\"dataset\"][\"file\"])\n",
    "        experiment_params.append({\"name\": exp, \"params\": params})     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an experiment\n",
    "\n",
    "We load the given dataset, and create trainers (which will be used for generating the policies for the logger and target). To try different trainers, modify the `log_trainer` and `tgt_trainer` variables with different `LinearTrainer`s. \n",
    "\n",
    "Note that DM's performance is highly dependent on the reward model. To try different reward models, modify the trainer passed into `DMEstimator` and `DoublyRobustEstimator` with different `LinearTrainer`s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_noisy(\n",
    "    experiments: Iterable[Tuple[Iterable[Estimator], int]],\n",
    "    dataset: UCIMultiClassDataset,\n",
    "    log_trainer: Trainer,\n",
    "    log_epsilon: float,\n",
    "    tgt_trainer: Trainer,\n",
    "    tgt_epsilon: float,\n",
    "    max_num_workers: int,\n",
    "    random_reward_prob: float = 0.0,\n",
    "    device=None,\n",
    "):\n",
    "    action_space = ActionSpace(dataset.num_actions)\n",
    "    config_path = PurePath(dataset.config_file)\n",
    "    data_name = config_path.stem\n",
    "    log_model_name = data_name + \"_\" + log_trainer.__class__.__name__ + \".pickle\"\n",
    "    log_model_file = str(config_path.with_name(log_model_name))\n",
    "    tgt_model_name = data_name + \"_\" + tgt_trainer.__class__.__name__ + \".pickle\"\n",
    "    tgt_model_file = str(config_path.with_name(tgt_model_name))\n",
    "\n",
    "    #log_trainer.load_model(log_model_file)\n",
    "    #tgt_trainer.load_model(tgt_model_file)\n",
    "    if not log_trainer.is_trained or not tgt_trainer.is_trained:\n",
    "        (\n",
    "            train_x,\n",
    "            train_y,\n",
    "            train_r,\n",
    "            val_x,\n",
    "            val_y,\n",
    "            val_r,\n",
    "            test_x,\n",
    "            test_y,\n",
    "            test_r,\n",
    "            train_choices,\n",
    "        ) = dataset.train_val_test_split((0.5, 0.8))\n",
    "        trainer_data = TrainingData(train_x, train_y, None, val_x, val_y, None)\n",
    "        #if not log_trainer.is_trained:\n",
    "        #    log_trainer.train(trainer_data)\n",
    "        #    log_trainer.save_model(log_model_file)\n",
    "        if not tgt_trainer.is_trained:\n",
    "            tgt_trainer.train(trainer_data)\n",
    "            tgt_trainer.save_model(tgt_model_file)\n",
    "            \n",
    "            \n",
    "    tgt_results = tgt_trainer.predict(dataset.features)\n",
    "    assert tgt_results.probabilities is not None\n",
    "    tgt_policy = MultiClassPolicy(action_space, tgt_results.probabilities, tgt_epsilon)\n",
    "    \n",
    "    #log_results = log_trainer.predict(dataset.features)\n",
    "    #assert log_results.probabilities is not None\n",
    "    uniform = torch.full(tgt_results.probabilities.shape, 1.0 / len(action_space))\n",
    "    #log_policy = MultiClassPolicy(action_space, log_results.probabilities, log_epsilon)\n",
    "    log_policy = MultiClassPolicy(action_space, uniform, log_epsilon)\n",
    "\n",
    "    tasks = []\n",
    "    test_queries = list(set(range(len(dataset))) - set(train_choices))\n",
    "    for estimators, num_samples in experiments:\n",
    "        samples = []\n",
    "        for _ in range(num_samples):\n",
    "            qid = random.sample(test_queries, 1)\n",
    "            label = int(dataset.labels[qid].item())\n",
    "            log_action, log_action_probabilities = log_policy(qid)\n",
    "            log_reward = 1.0 if log_action.value == label else 0.0\n",
    "            tgt_action, tgt_action_probabilities = tgt_policy(qid)\n",
    "            ground_truth_reward = 1.0 if tgt_action.value == label else 0.0\n",
    "            item_feature = dataset.features[qid]\n",
    "            random_reward = random.random() < random_reward_prob\n",
    "            samples.append(\n",
    "                LogSample(\n",
    "                    context=qid,\n",
    "                    log_action=log_action,\n",
    "                    log_reward=random.randint(0, 1) if random_reward else log_reward,\n",
    "                    log_action_probabilities=log_action_probabilities,\n",
    "                    tgt_action_probabilities=tgt_action_probabilities,\n",
    "                    tgt_action=tgt_action,\n",
    "                    ground_truth_reward=ground_truth_reward,\n",
    "                    item_feature=item_feature,\n",
    "                )\n",
    "            )\n",
    "        tasks.append((estimators, BanditsEstimatorInput(action_space, samples, False)))\n",
    "\n",
    "    evaluator = Evaluator(tasks, max_num_workers)\n",
    "    results = evaluator.evaluate()\n",
    "    Evaluator.report_results(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(params):\n",
    "        return UCIMultiClassDataset(params[\"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment(s)\n",
    "def run_experiment(dataset): \n",
    "    random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "    torch.random.manual_seed(1234)\n",
    "\n",
    "    log_trainer = LogisticRegressionTrainer()\n",
    "    log_epsilon = 0.1\n",
    "    tgt_trainer = SGDClassifierTrainer()\n",
    "    tgt_epsilon = 0.1\n",
    "    experiments = [\n",
    "        (\n",
    "            (\n",
    "                SwitchEstimator(LogisticRegressionTrainer(), rmax=1.0),\n",
    "                SwitchDREstimator(LogisticRegressionTrainer(), rmax=1.0),\n",
    "                DMEstimator(LogisticRegressionTrainer()),\n",
    "                IPSEstimator(),\n",
    "                DoublyRobustEstimator(LogisticRegressionTrainer()),\n",
    "            ),\n",
    "            1000,\n",
    "        )\n",
    "        for _ in range(100)\n",
    "    ]\n",
    "    results = evaluate_all_noisy(\n",
    "        experiments, dataset, log_trainer, log_epsilon, tgt_trainer, tgt_epsilon, 0, 0.5\n",
    "    )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Generation\n",
    "\n",
    "For each UCI dataset, we generate a logging and target policy, create a simulated dataset using the logging policy, and evaluate the target policy using DM, IPS, and DR. The bias, rmse, and variance against the ground truth is plotted for each dataset. \n",
    "\n",
    "\n",
    "For the settings with the logging policy trained with a `LogisticRegressionTrainer`, the target policy with a `SGDClassifierTrainer`, and the reward model for DM and DR trained with a `LogisticRegressionTrainer`, a sample result gives:\n",
    "\n",
    "\n",
    "![alt text](img/bias.png \"Bias\")![alt text](img/variance.png \"Bias\")![alt text](img/rmse.png \"Bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for params in experiment_params:\n",
    "    datasets.append(load_dataset(params['params']))\n",
    "labels = []\n",
    "\n",
    "bias_result_mapping = {}\n",
    "var_result_mapping = {}\n",
    "rmse_result_mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment ecoli\n",
      "SwitchEstimator(trainer(logistic_regression),weight_clamper(Clamper(-inf,inf)),device(None)) rewards: log_reward0.3145700000000001 tgt_reward[0.5792702929675579] gt_reward[0.6479199999999998], diffs: tgt-gt[samples=100, rmse=0.08826843444395464, bias=-0.06864970703244203, variance=0.003109630549036968] tgt-log[samples=100, rmse=0.269050868616176, bias=0.2647002929675578, variance=0.0023455806121291606]\n",
      "SwitchDREstimator(trainer(logistic_regression),weight_clamper(Clamper(-inf,inf)),device(None)) rewards: log_reward0.3145700000000001 tgt_reward[0.5768058840930462] gt_reward[0.6479199999999998], diffs: tgt-gt[samples=100, rmse=0.08041803618813323, bias=-0.07111411590695375, variance=0.001424083902149753] tgt-log[samples=100, rmse=0.26409560671470594, bias=0.2622358840930461, variance=0.000988717757522366]\n",
      "DMEstimator(trainer(logistic_regression,device(None)) rewards: log_reward0.3145700000000001 tgt_reward[0.4787884335635231] gt_reward[0.6479199999999998], diffs: tgt-gt[samples=100, rmse=0.17374743909282628, bias=-0.16913156643647692, variance=0.0015986725515747208] tgt-log[samples=100, rmse=0.16893882673376942, bias=0.16421843356352292, variance=0.0015885184405306798]\n",
      "IPSEstimator(weight_clamper(Clamper(-inf,inf)),weighted(False),device(None)) rewards: log_reward0.3145700000000001 tgt_reward[0.579270295387581] gt_reward[0.6479199999999998], diffs: tgt-gt[samples=100, rmse=0.08826843277572166, bias=-0.06864970461241898, variance=0.003109630587181532] tgt-log[samples=100, rmse=0.2690508713082726, bias=0.2647002953875809, variance=0.002345580781280917]\n",
      "DoublyRobustEstimator(trainer(logistic_regression),weight_clamper(Clamper(-inf,inf)),device(None)) rewards: log_reward0.3145700000000001 tgt_reward[0.5776792460019063] gt_reward[0.6479199999999998], diffs: tgt-gt[samples=100, rmse=0.08004570592961396, bias=-0.07024075399809349, variance=0.0014884358742924763] tgt-log[samples=100, rmse=0.2650540728811515, bias=0.2631092460019064, variance=0.0010375618375708183]\n",
      "Running experiment letter_recog\n"
     ]
    }
   ],
   "source": [
    "for dataset, params in zip(datasets, experiment_params):\n",
    "    print(\"Running experiment \" + params[\"name\"])\n",
    "    if params[\"name\"] in labels:\n",
    "        continue\n",
    "    exp_results = run_experiment(dataset)\n",
    "    labels.append(params[\"name\"])\n",
    "\n",
    "    for estimator_name, result in exp_results.items():\n",
    "        _, _, _, tgt_gt, _, _ = result.report()\n",
    "        result_var = torch.tensor(\n",
    "            [res.estimated_reward for res in result.results],\n",
    "            dtype=torch.double,\n",
    "        ).var().item()\n",
    "        if not estimator_name in bias_result_mapping:\n",
    "            bias_result_mapping[estimator_name] = []\n",
    "        if not estimator_name in var_result_mapping:\n",
    "            var_result_mapping[estimator_name] = []\n",
    "        if not estimator_name in rmse_result_mapping:\n",
    "            rmse_result_mapping[estimator_name] = []\n",
    "\n",
    "        bias_result_mapping[estimator_name].append(tgt_gt.bias.cpu().numpy())\n",
    "        var_result_mapping[estimator_name].append(result_var)\n",
    "        rmse_result_mapping[estimator_name].append(tgt_gt.rmse.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Bar Charts, a la https://arxiv.org/pdf/1511.03722.pdf\n",
    "print(labels)\n",
    "def create_and_show_chart(labels, results, title):\n",
    "    # Width of each bar\n",
    "    width = 0.1\n",
    "\n",
    "    metrics = list(results.keys())\n",
    "    \n",
    "    # Set position of bar on X axis\n",
    "    barpos = [np.arange(len(results[metrics[0]]))]\n",
    "    for m in range(len(metrics)-1):\n",
    "        barpos.append([x + width for x in barpos[-1]])\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    for metric, barpositions in zip(metrics, barpos):\n",
    "        ax.bar(barpositions, results[metric], width, label=metric[:MAX_METRIC_NAME_LENGTH])\n",
    "\n",
    "    ax.set_ylabel(title)\n",
    "    plt.xticks([r + width for r in range(len(labels))], labels)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "create_and_show_chart(labels, bias_result_mapping, 'Bias')\n",
    "create_and_show_chart(labels, rmse_result_mapping, 'RMSE')\n",
    "create_and_show_chart(labels, var_result_mapping, 'Variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
